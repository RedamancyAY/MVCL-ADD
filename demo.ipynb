{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from argparse import  Namespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build MVCL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ay/anaconda3/envs/mvcl/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from MVCL import MultiViewModel_lit, MultiViewModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Configuration Parameters\n",
    "\n",
    "| Parameter | Type | Default | Description |\n",
    "|-----------|------|---------|-------------|\n",
    "| `use_inner_CL` | int | 1 | Enable **inner contrastive learning** within the same modality for feature discrimination |\n",
    "| `use_inter_CL` | int | 1 | Enable **inter-modal contrastive learning** between different modalities |\n",
    "| `use_cls_loss_1_2` | int | 1 | Enable classification loss for modality 1 and modality 2 tasks |\n",
    "| `use_fusion` | int | 1 | Enable **feature fusion mechanism** to combine multi-modal features |\n",
    "| `use_fusion1D` | int | 1 | Enable **1D fusion** strategy for processing sequential feature fusion |\n",
    "| `use_fusion2D` | int | 1 | Enable **2D fusion** strategy for processing spatial feature map fusion |\n",
    "| `use_mse_loss` | int | 0 | Enable Mean Squared Error loss for regression tasks |\n",
    "| `only_1D` | int | 0 | **Use only 1D modality**, ignoring other dimensional features |\n",
    "| `only_2D` | int | 0 | **Use only 2D modality**, ignoring other dimensional features |\n",
    "| `drop_layer` | float | 0.0 | Dropout rate for regularization to prevent overfitting |\n",
    "| `w_con` | float | 1.0 | Weight coefficient for contrastive learning loss in total loss |\n",
    "| `w_cls` | float | 1.0 | Weight coefficient for classification loss in total loss |\n",
    "\n",
    "### Parameter Categories\n",
    "\n",
    "#### 🎯 **Loss Function Control**\n",
    "- `use_inner_CL`, `use_inter_CL`: Control different types of contrastive learning\n",
    "- `use_cls_loss_1_2`: Control classification loss\n",
    "- `use_mse_loss`: Control regression loss\n",
    "\n",
    "#### 🔄 **Feature Fusion Strategy**\n",
    "- `use_fusion`: Master switch for feature fusion\n",
    "- `use_fusion1D`, `use_fusion2D`: Control fusion methods for different dimensions\n",
    "\n",
    "#### 🎛️ **Modality Selection**\n",
    "- `only_1D`, `only_2D`: Control whether to use only specific dimensional modalities\n",
    "\n",
    "#### ⚖️ **Weight Balancing**\n",
    "- `w_con`, `w_cls`: Balance the importance of different loss functions\n",
    "- `drop_layer`: Regularization parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### default model configuration\n",
    "mvcl_cfg = Namespace(\n",
    "    use_inner_CL=1,\n",
    "    use_inter_CL=1,\n",
    "    use_cls_loss_1_2=1,\n",
    "    use_fusion=1,\n",
    "    use_fusion1D=1,\n",
    "    use_fusion2D=1,\n",
    "    use_mse_loss=0,\n",
    "    only_1D=0,\n",
    "    only_2D=0,\n",
    "    drop_layer=0.0,\n",
    "    w_con=1.0,\n",
    "    w_cls=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first building, this will download the Wav2Clip model checkpoints and the WavLM model checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mvcl = MultiViewModel(cfg=mvcl_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input of our MVCL model is a torch Tensor with shape of (batch, 1, audio_length).\n",
    "\n",
    "Take an random tensor as example. The batch size is 2, indicating this output is from processing 2 audio samples simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 1, 48000)\n",
    "res = mvcl(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_spec: torch.Size([2, 1, 257, 257])\n",
      "raw_wav_feat: torch.Size([2, 149, 768])\n",
      "feature1D: torch.Size([2, 768])\n",
      "feature2D: torch.Size([2, 512])\n",
      "feature: torch.Size([2, 1280])\n",
      "logit1D: torch.Size([2])\n",
      "logit2D: torch.Size([2])\n",
      "logit: torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "for k, v in res.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        print(f\"{k}: {v.shape}\")\n",
    "    else:\n",
    "        print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is a dict:\n",
    "\n",
    "| Feature Name | Shape | Dimension | Description |\n",
    "|--------------|-------|-----------|-------------|\n",
    "| `raw_spec` | `[2, 1, 257, 257]` | 4D | **Raw spectrogram** - Original frequency-time representation of audio signal with 257 frequency bins and 257 time frames |\n",
    "| `raw_wav_feat` | `[2, 149, 768]` | 3D | **Raw waveform features** - Sequential audio features extracted from backbone (e.g., WavLM), 149 time steps with 768-dimensional embeddings |\n",
    "| `feature1D` | `[2, 768]` | 2D | **1D modality features** - the final classification feat of the 1D branch |\n",
    "| `feature2D` | `[2, 512]` | 2D | **2D modality features** - the final classification feat of the 2D branch |\n",
    "| `feature` | `[2, 1280]` | 2D | **Fused features** - Combined multi-modal features (1D + 2D), concatenated to 1280 dimensions (768 + 512) |\n",
    "| `logit1D` | `[2]` | 1D | **1D modality logits** - Classification scores from 1D feature branch for binary classification |\n",
    "| `logit2D` | `[2]` | 1D | **2D modality logits** - Classification scores from 2D feature branch for binary classification |\n",
    "| `logit` | `[2]` | 1D | **Final logits** - Combined classification scores from fused features for final prediction |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Feature Processing Pipeline**\n",
    "\n",
    "```\n",
    "Audio Input (batch, 1, 48000)\n",
    "    ↓\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                    Stage 1 (No Grad)                       │\n",
    "├──────────────────────────┬──────────────────────────────────┤\n",
    "│      1D Branch           │           2D Branch              │\n",
    "│                          │                                  │\n",
    "│ feature_model1D          │ feature_model2D                  │\n",
    "│ .compute_stage1(x)       │ .compute_stage1(x, spec_aug)     │\n",
    "│      ↓                   │      ↓                           │\n",
    "│    wav1                  │   spec1, raw_spec               │\n",
    "└──────────────────────────┴──────────────────────────────────┘\n",
    "                           ↓\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                 Cross-Modal Fusion                          │\n",
    "├──────────────────────────┬──────────────────────────────────┤\n",
    "│   squeeze_modules[0]     │     expand_modules[0]            │\n",
    "│   (wav1, spec1)          │     (wav1, spec1)                │\n",
    "│      ↓                   │      ↓                           │\n",
    "│   fused_wav1             │   fused_spec1                    │\n",
    "└──────────────────────────┴──────────────────────────────────┘\n",
    "                           ↓\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                    Stage 2                                  │\n",
    "├──────────────────────────┬──────────────────────────────────┤\n",
    "│ feature_model1D          │ feature_model2D                  │\n",
    "│ .compute_stage2          │ .compute_stage2                  │\n",
    "│ (fused_wav1)             │ (fused_spec1)                    │\n",
    "│      ↓                   │      ↓                           │\n",
    "│ wav2, position_bias      │   spec2                          │\n",
    "└──────────────────────────┴──────────────────────────────────┘\n",
    "                           ↓\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│              Cross-Modal Fusion + Stage 3                   │\n",
    "├──────────────────────────┬──────────────────────────────────┤\n",
    "│   squeeze_modules[1]     │     expand_modules[1]            │\n",
    "│   (wav2, spec2)          │     (wav2, spec2)                │\n",
    "│      ↓                   │      ↓                           │\n",
    "│ feature_model1D          │ feature_model2D                  │\n",
    "│ .compute_stage3          │ .compute_stage3                  │\n",
    "│      ↓                   │      ↓                           │\n",
    "│ wav3, position_bias      │   spec3                          │\n",
    "└──────────────────────────┴──────────────────────────────────┘\n",
    "                           ↓\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│              Cross-Modal Fusion + Stage 4                   │\n",
    "├──────────────────────────┬──────────────────────────────────┤\n",
    "│   squeeze_modules[2]     │     expand_modules[2]            │\n",
    "│   (wav3, spec3)          │     (wav3, spec3)                │\n",
    "│      ↓                   │      ↓                           │\n",
    "│ feature_model1D          │ feature_model2D                  │\n",
    "│ .compute_stage4          │ .compute_stage4                  │\n",
    "│      ↓                   │      ↓                           │\n",
    "│ wav4, position_bias      │   spec4                          │\n",
    "└──────────────────────────┴──────────────────────────────────┘\n",
    "                           ↓\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│           Final Cross-Modal Fusion + Latent Features        │\n",
    "├──────────────────────────┬──────────────────────────────────┤\n",
    "│   squeeze_modules[3]     │     expand_modules[3]            │\n",
    "│   (wav4, spec4)          │     (wav4, spec4)                │\n",
    "│      ↓                   │      ↓                           │\n",
    "│ feature_model1D          │ feature_model2D                  │\n",
    "│ .compute_latent_feature  │ .compute_latent_feature          │\n",
    "│      ↓                   │      ↓                           │\n",
    "│ wav5, raw_wav_feat       │   spec5                          │\n",
    "└──────────────────────────┴──────────────────────────────────┘\n",
    "                           ↓\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                Feature Normalization                        │\n",
    "├──────────────────────────┬──────────────────────────────────┤\n",
    "│   norm_feat(wav5)        │   norm_feat(spec5)               │\n",
    "│      ↓                   │      ↓                           │\n",
    "│   feature1D [B, 768]     │   feature2D [B, 512]             │\n",
    "│      ↓                   │      ↓                           │\n",
    "│   cls1D(feature1D)       │   cls2D(feature2D)               │\n",
    "│      ↓                   │      ↓                           │\n",
    "│   logit1D [B]            │   logit2D [B]                    │\n",
    "└──────────────────────────┴──────────────────────────────────┘\n",
    "                           ↓\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                   Multi-Modal Fusion                        │\n",
    "│                                                             │\n",
    "│        concat([wav5, spec5], dim=-1)                        │\n",
    "│                     ↓                                       │\n",
    "│              norm_feat(concat)                              │\n",
    "│                     ↓                                       │\n",
    "│               feature [B, 1280]                             │\n",
    "│                     ↓                                       │\n",
    "│              cls_final(feature)                             │\n",
    "│                     ↓                                       │\n",
    "│                logit [B]                                    │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lit model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> PyTorch Lightning is the deep learning framework for professional AI researchers and machine learning engineers who need maximal flexibility without sacrificing performance at scale. Lightning evolves with you as your projects go from idea to paper/production\n",
    "\n",
    "We use [pytorch_lightning](https://lightning.ai/docs/pytorch/stable/) to train, validate, and test our model. Besides, it can also easily control the logging, model saving and callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer, LightningModule\n",
    "from pytorch_lightning.loggers import  CSVLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the pytorch Lightning module to train the model, where we define the train step, validation/predict step, loss function and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCE loss with label smoothing:  0.1\n"
     ]
    }
   ],
   "source": [
    "mvcl_lit = MultiViewModel_lit(cfg=mvcl_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test forwarding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lit model, we use the `_shared_pred` method to predict the logits of the input batch. If the stage is train, we also the the audio_transform to augment the spectrogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a random batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = {\n",
    "    \"label\": torch.randint(0, 2, (3,)),\n",
    "    \"audio\": torch.randn(3, 1, 48000),\n",
    "    \"sample_rate\": [16000, 16000, 16000],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, **your batch must be a dict with above keys**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the `_shared_pred` output is also a dict. We use it to compute the loss\n",
    "function, AUC, and ERR scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_spec torch.Size([3, 1, 257, 257])\n",
      "raw_wav_feat torch.Size([3, 149, 768])\n",
      "feature1D torch.Size([3, 768])\n",
      "feature2D torch.Size([3, 512])\n",
      "feature torch.Size([3, 1280])\n",
      "logit1D torch.Size([3])\n",
      "logit2D torch.Size([3])\n",
      "logit torch.Size([3])\n",
      "pred torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "batch_res = mvcl_lit._shared_pred(batch=batch, batch_idx=0)\n",
    "for key, value in batch_res.items():\n",
    "    print(key, value.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first build a simple dataloaders for training, where all the samples are randomly generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from callbacks import EER_Callback, BinaryAUC_Callback, BinaryACC_Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a dataloader with random values for demo training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTestDataset(Dataset):\n",
    "    def __init__(self, num_samples=10):\n",
    "        # Generate synthetic data similar to your example\n",
    "        self.samples = []\n",
    "        for _ in range(num_samples):\n",
    "            self.samples.append({\n",
    "                \"audio\": torch.randn(1, 48000),\n",
    "                \"label\": torch.randint(0, 2, (1,)).item(),\n",
    "                \"sample_rate\": 16000,\n",
    "            })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "train_dataloader = DataLoader(\n",
    "    SimpleTestDataset(num_samples=100),\n",
    "    batch_size=3,\n",
    "    shuffle=True,\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    SimpleTestDataset(num_samples=50),\n",
    "    batch_size=3,\n",
    "    shuffle=False,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    SimpleTestDataset(num_samples=20),\n",
    "    batch_size=3,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a simple trainer to train and test our model, which uses:\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    logger=CSVLogger(save_dir=\"./logs\", version=0),\n",
    "    max_epochs=4,\n",
    "    callbacks=[\n",
    "        BinaryACC_Callback(batch_key=\"label\", output_key=\"logit\"),\n",
    "        BinaryAUC_Callback(batch_key=\"label\", output_key=\"logit\"),\n",
    "        EER_Callback(batch_key=\"label\", output_key=\"logit\"),\n",
    "    ],\n",
    "    devices=[0], # use cuda:0 device\n",
    "    accelerator=\"gpu\", # use GPU acceleration\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "/home/ay/anaconda3/envs/mvcl/lib/python3.9/site-packages/lightning_fabric/loggers/csv_logs.py:268: Experiment logs directory ./logs/lightning_logs/version_0 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "/home/ay/anaconda3/envs/mvcl/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:658: Checkpoint directory ./logs/lightning_logs/version_0/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name                | Type                            | Params | Mode \n",
      "--------------------------------------------------------------------------------\n",
      "0 | model               | MultiViewModel                  | 128 M  | train\n",
      "1 | clip_heads          | ModuleList                      | 1.6 M  | train\n",
      "2 | bce_loss            | LabelSmoothingBCE               | 0      | train\n",
      "3 | contrast_loss2      | BinaryTokenContrastLoss         | 0      | train\n",
      "4 | triplet_loss        | TripletMarginLoss               | 0      | train\n",
      "5 | clip_loss           | CLIPLoss1D                      | 1      | train\n",
      "6 | reconstruction_loss | TimeFrequencyReconstructionLoss | 379 K  | train\n",
      "--------------------------------------------------------------------------------\n",
      "130 M     Trainable params\n",
      "0         Non-trainable params\n",
      "130 M     Total params\n",
      "521.061   Total estimated model params size (MB)\n",
      "210       Modules in train mode\n",
      "233       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ay/anaconda3/envs/mvcl/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ay/anaconda3/envs/mvcl/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.\n",
      "/home/ay/anaconda3/envs/mvcl/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (34) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 34/34 [00:03<00:00,  8.89it/s, v_num=0, val-clip_loss=2.200, val-mse_loss=1.000, val-cls_loss1D=0.688, val-cls_loss2D=0.692, val-cls_loss=0.689, val-contrast_loss=0.310, val-contrast_loss1D=0.330, val-contrast_loss2D=0.284, val-loss=4.880, val-acc=0.440, val-auc=0.458, val-eer=0.545, train-clip_loss=2.250, train-mse_loss=1.000, train-cls_loss1D=0.690, train-cls_loss2D=0.688, train-cls_loss=0.686, train-contrast_loss=0.296, train-contrast_loss1D=0.336, train-contrast_loss2D=0.276, train-loss=4.930, train-acc=0.440, train-auc=0.524, train-eer=0.536]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=4` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 34/34 [00:09<00:00,  3.73it/s, v_num=0, val-clip_loss=2.200, val-mse_loss=1.000, val-cls_loss1D=0.688, val-cls_loss2D=0.692, val-cls_loss=0.689, val-contrast_loss=0.310, val-contrast_loss1D=0.330, val-contrast_loss2D=0.284, val-loss=4.880, val-acc=0.440, val-auc=0.458, val-eer=0.545, train-clip_loss=2.250, train-mse_loss=1.000, train-cls_loss1D=0.690, train-cls_loss2D=0.688, train-cls_loss=0.686, train-contrast_loss=0.296, train-contrast_loss1D=0.336, train-contrast_loss2D=0.276, train-loss=4.930, train-acc=0.440, train-auc=0.524, train-eer=0.536]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(mvcl_lit, train_dataloader, val_dataloaders=val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, you can view the logging loss in the logger file, for example `logs/lightning_logs/version_0/metrics.csv`.\n",
    "![](imgs/loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After testing, the results will also saved in logger file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "/home/ay/anaconda3/envs/mvcl/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 7/7 [00:00<00:00, 29.50it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test-acc            0.44999998807907104\n",
      "        test-auc            0.4545454978942871\n",
      "     test-clip_loss         2.1567835807800293\n",
      "      test-cls_loss         0.6900644302368164\n",
      "     test-cls_loss1D        0.6895066499710083\n",
      "     test-cls_loss2D        0.6917417049407959\n",
      "   test-contrast_loss       0.31804102659225464\n",
      "  test-contrast_loss1D      0.34499993920326233\n",
      "  test-contrast_loss2D      0.28468912839889526\n",
      "        test-eer            0.5454545617103577\n",
      "        test-loss            4.857785701751709\n",
      "      test-mse_loss         1.0041100978851318\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test-clip_loss': 2.1567835807800293,\n",
       "  'test-mse_loss': 1.0041100978851318,\n",
       "  'test-cls_loss1D': 0.6895066499710083,\n",
       "  'test-cls_loss2D': 0.6917417049407959,\n",
       "  'test-cls_loss': 0.6900644302368164,\n",
       "  'test-contrast_loss': 0.31804102659225464,\n",
       "  'test-contrast_loss1D': 0.34499993920326233,\n",
       "  'test-contrast_loss2D': 0.28468912839889526,\n",
       "  'test-loss': 4.857785701751709,\n",
       "  'test-acc': 0.44999998807907104,\n",
       "  'test-auc': 0.4545454978942871,\n",
       "  'test-eer': 0.5454545617103577}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(mvcl_lit, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Note, train, val, and test process will logging in the same file: `metrics.csv`.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "mvcl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
